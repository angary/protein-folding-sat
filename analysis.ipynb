{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import tikzplotlib\n",
    "\n",
    "from src.config import SAT_TEST_SEQ, POLICIES\n",
    "\n",
    "COLOURS = [\"blue\", \"orange\", \"green\", \"red\"]\n",
    "ENCODING_DIR = \"./results/encoding\"\n",
    "POLICY_DIR = \"./results/policy\"\n",
    "SOLVER_DIR = \"./results/solver\"\n",
    "TEX_DIR = \"./tex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle what results to show\n",
    "SHOW_ENCODING_RESULTS = True\n",
    "SHOW_SAT_RESULTS = True\n",
    "SHOW_SOLVER_RESULTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the results from a directory\n",
    "def get_result_dicts(dir_path: str) -> list[dict[str, float | str]]:\n",
    "    results = []\n",
    "    for file_name in filter(lambda x: x.endswith(\".csv\"), os.listdir(dir_path)):\n",
    "        # Open the csv file\n",
    "        file_dict = pd.read_csv(os.path.join(dir_path, file_name)).to_dict()\n",
    "        # For each header in the csv where (key1=name,val1=dict(key=row_num,val=val_at_row))\n",
    "        valid = True\n",
    "        for key1, val1 in file_dict.items():\n",
    "            # If the value is a string, convert from a dict of strings to 1 string\n",
    "            if len(val1) == 0:\n",
    "                valid = False\n",
    "                break\n",
    "            if type(val1[0]) == str:\n",
    "                file_dict[key1] = val1[0]\n",
    "            # Else find the average of the values\n",
    "            else:\n",
    "                values = [v for v in val1.values()]\n",
    "                file_dict[key1] = sum(values) / len(values)\n",
    "        if valid:\n",
    "            results.append(file_dict)\n",
    "    return results\n",
    "\n",
    "# Helper functions for working with return value of the above function\n",
    "def get_all_vars_for(result_dicts: list[dict[str, float | str]], var_name: str) -> list:\n",
    "    return sorted(set(x[var_name] for x in result_dicts))\n",
    "\n",
    "def filter_by(result_dicts: list[dict[str, float | str]], var_name: str, var: str | float) -> list:\n",
    "    return [x for x in result_dicts if x[var_name] == var]\n",
    "\n",
    "# Given the results, plot the change in time per length for the independent variable\n",
    "def plot_data(result_dicts: list[dict[str, float | str]], indp_var: str):\n",
    "    # TODO: Complete function to plot data to compare indp vars, where x axis is length, y axis is time taken\n",
    "    mapped_results: dict[str, list[dict]] = {}\n",
    "    fig, ax_left = plt.subplots(figsize=(16, 10))\n",
    "    ax_right = ax_left.twinx()\n",
    "\n",
    "    for r in result_dicts:\n",
    "        key = r[indp_var]\n",
    "        mapped_results[key] = mapped_results.get(key, []) + [r]\n",
    "\n",
    "    # Loop through each \n",
    "    for i, (var, results) in enumerate(mapped_results.items()):\n",
    "        data = []\n",
    "        incl = [\"len\", \"encode_time\", \"total_time\", \"sat_time\"]\n",
    "        for result in results:\n",
    "            data.append({i: result[i] for i in incl})\n",
    "        data.sort(key=lambda x:x[\"len\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Test Results\n",
    "\n",
    "The following code compares for each policy the best combination of `[search-policy][sat-solver][encoding-version]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_ENCODING_RESULTS:\n",
    "    #  Compare the performance of each (policy, solver, version) combination\n",
    "    results = get_result_dicts(POLICY_DIR)\n",
    "    policies = get_all_vars_for(results, \"policy\")\n",
    "    solvers = get_all_vars_for(results, \"solver\")\n",
    "    vers = get_all_vars_for(results, \"ver\")\n",
    "    lengths = get_all_vars_for(results, \"len\")[:-1] # Ignore last length since not everything was tested\n",
    "    print(lengths)\n",
    "\n",
    "    # Results\n",
    "    results_each_len: dict[str, dict[str, dict[list[float]]]] = {\n",
    "        policy: {\n",
    "            solver: {\n",
    "                ver: [] for ver in vers\n",
    "            } for solver in solvers\n",
    "        } for policy in policies\n",
    "    }\n",
    "\n",
    "    print(\" \" * (1 + len(policies[0]) + len(solvers[0])), \"   \\t\", \"[v0,      v1,     v2]\")\n",
    "    policy_times_per_len = {\n",
    "        policy: {\n",
    "            length: [] for length in lengths\n",
    "        } for policy in policies\n",
    "    }\n",
    "\n",
    "    # Get the times for all policies\n",
    "    for policy in policies:\n",
    "        policy_times = []\n",
    "        # Get the times for all solvers\n",
    "        for solver in solvers:\n",
    "            policy_solver_results = filter_by(filter_by(results, \"policy\", policy), \"solver\", solver)\n",
    "            ver_results = []\n",
    "            # Get the time for all versions\n",
    "            for ver in vers:\n",
    "                policy_solver_ver_results = filter_by(policy_solver_results, \"ver\", ver)\n",
    "                avg_ver_results = np.mean([x[\"total_time\"] for x in policy_solver_ver_results])\n",
    "                ver_results.append(round(avg_ver_results, 4))\n",
    "\n",
    "                # Get the time for all lengths\n",
    "                for length in lengths:\n",
    "                    length_results = filter_by(policy_solver_ver_results, \"len\", length)\n",
    "                    results_each_len[policy][solver][ver].append(np.mean([x[\"total_time\"] for x in length_results]))\n",
    "                    avg_times = np.mean([x[\"total_time\"] for x in length_results])\n",
    "                    policy_times_per_len[policy][length].append(avg_times)\n",
    "            \n",
    "            # Print out the times for current policy and solver\n",
    "            ver_avg = round(np.mean(ver_results), 4)\n",
    "            policy_times.append(ver_avg)\n",
    "            print(policy, solver, \"   \\t\", ver_results, f\"\\t{ver_avg = }\")\n",
    "        \n",
    "        # Print out the average time for the policy\n",
    "        policy_avg = round(np.mean(policy_times), 4)\n",
    "        print(f\"{policy_avg = }\")\n",
    "        print()\n",
    "\n",
    "    # Plot the times taken\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    for p in policies:\n",
    "        # TODO: Show average for all solvers and all encodings\n",
    "        times = []\n",
    "\n",
    "        for l in lengths:\n",
    "            times.append(np.mean(policy_times_per_len[p][l]))\n",
    "        \n",
    "        plt.plot(lengths, times, label=p.replace(\"_\", \" \"))\n",
    "    plt.legend()\n",
    "    scale_mode = \"linear\"\n",
    "    plt.yscale(scale_mode)\n",
    "    tikzplotlib.save(os.path.join(TEX_DIR, f\"policy-{scale_mode}.tex\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAT Test Results\n",
    "\n",
    "The following code shows how long it takes for a kissat to solve for different goal contacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_SAT_RESULTS:\n",
    "    with open(SAT_TEST_SEQ.replace(\"input\", \"results/sat\") + \".json\") as f:\n",
    "        data = json.load(f)\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        times = data[\"times\"]\n",
    "        n = len(times)\n",
    "        calls = [i + 1 for i in range(n)]\n",
    "        plt.bar(calls, times)\n",
    "        # plt.yscale(\"log\")\n",
    "        tikzplotlib.save(os.path.join(TEX_DIR, \"sat.tex\"))\n",
    "        plt.show()\n",
    "        print(f\"{times = }\")\n",
    "        for policy in POLICIES:\n",
    "            calls = data[policy]\n",
    "            total = sum([times[i - 1] for i in calls])\n",
    "            print(policy, total, len(calls), calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver Test Results\n",
    "\n",
    "The following code compares how long it takes to solve for different combinations of solvers and encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_SOLVER_RESULTS:\n",
    "    #  Compare the performance of each (solver, version) combination\n",
    "    results = get_result_dicts(SOLVER_DIR)\n",
    "    solvers = get_all_vars_for(results, \"solver\")\n",
    "    vers = get_all_vars_for(results, \"ver\")\n",
    "    lengths = get_all_vars_for(results, \"len\")[:-1]\n",
    "\n",
    "    # Results\n",
    "    results_each_len: dict[str, dict[str, dict[list[float]]]] = {\n",
    "        solver: {\n",
    "            ver: [] for ver in vers\n",
    "        } for solver in solvers\n",
    "    }\n",
    "\n",
    "    solver_times_per_len = {\n",
    "        solver: {\n",
    "            length: [] for length in lengths\n",
    "        } for solver in solvers\n",
    "    }\n",
    "\n",
    "    print(\" \" * (1 + len(solvers[0])), \"   \\t\", \"[v0,      v1,     v2]\")\n",
    "    # Get the times for all solvers\n",
    "    for solver in solvers:\n",
    "        solver_results = filter_by(results, \"solver\", solver)\n",
    "        ver_results = []\n",
    "        # Get the time for all versions\n",
    "        for ver in vers:\n",
    "            solver_ver_results = filter_by(solver_results, \"ver\", ver)\n",
    "            avg_ver_results = np.mean(\n",
    "                [x[\"total_time\"] for x in solver_ver_results])\n",
    "            ver_results.append(round(avg_ver_results, 4))\n",
    "\n",
    "            # Get the time for all lengths\n",
    "            for length in lengths:\n",
    "                length_results = filter_by(\n",
    "                    solver_ver_results, \"len\", length)\n",
    "                avg_times = np.mean([x[\"total_time\"] for x in length_results])\n",
    "                results_each_len[solver][ver].append(avg_times)\n",
    "                solver_times_per_len[solver][length].append(avg_times)\n",
    "\n",
    "        # Print out the times for current policy and solver\n",
    "        ver_avg = round(np.mean(ver_results), 4)\n",
    "        print(solver, \"   \\t\", ver_results, f\"\\t{ver_avg = }\")\n",
    "\n",
    "    # Plot the times taken\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    for s in solvers:\n",
    "        times = []\n",
    "        for l in lengths:\n",
    "            times.append(np.mean(solver_times_per_len[s][l]))\n",
    "        plt.plot(lengths, times, label=s)\n",
    "    plt.legend()\n",
    "    scale_mode = \"linear\"\n",
    "    plt.yscale(scale_mode)\n",
    "    tikzplotlib.save(os.path.join(TEX_DIR, f\"solver-{scale_mode}.tex\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Test Results\n",
    "\n",
    "The following code compares how long it takes to solve for the different encoding versions to solve 3D instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 3\n",
    "yscale_mode = \"linear\"\n",
    "results = filter_by(get_result_dicts(ENCODING_DIR), \"dim\", dimension)\n",
    "vers = get_all_vars_for(results, \"ver\")\n",
    "lengths = get_all_vars_for(results, \"len\")[:-1]\n",
    "dims = get_all_vars_for(results, \"dim\")\n",
    "\n",
    "fig, ax_left = plt.subplots(figsize=(16, 10))\n",
    "ax_right = ax_left.twinx()\n",
    "for i, ver in enumerate(vers):\n",
    "    ver_results = filter_by(results, \"ver\", ver)\n",
    "    length_times = []\n",
    "    length_cls = []\n",
    "    length_vars = []\n",
    "    for length in lengths:\n",
    "        ver_len_result = filter_by(ver_results, \"len\", length)\n",
    "        times = [x[\"total_time\"] for x in ver_len_result]\n",
    "        cls = [x[\"cls\"] for x in ver_len_result]\n",
    "        vars = [x[\"vars\"] for x in ver_len_result]\n",
    "        length_times.append(np.mean(times))\n",
    "        length_cls.append(np.mean(cls))\n",
    "        length_vars.append(np.mean(vars))\n",
    "    ax_left.plot(lengths, length_times, color=COLOURS[i], marker=\"o\", label=f\"{ver} times\")\n",
    "    ax_right.plot(lengths, length_cls, \":\", color=COLOURS[i], label=f\"{ver} clauses\")\n",
    "    ax_right.plot(lengths, length_vars, \"--\", color=COLOURS[i], label=f\"{ver} variables\")\n",
    "\n",
    "ax_left.set_yscale(yscale_mode)\n",
    "ax_left.set_ylabel(\"Time (s)\")\n",
    "\n",
    "ax_right.set_yscale(yscale_mode)\n",
    "ax_right.set_ylabel(\"Variables / Clauses\")\n",
    "# ax_left.grid(True, which=\"minor\")\n",
    "ax_left.set_xlabel(\"Sequence Length\")\n",
    "\n",
    "ax_left.legend()\n",
    "ax_right.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0a5145e6c304e2a9afaf5b930a2955b950bd4b81fe94f7c42930f43f42762eb"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
